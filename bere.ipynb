{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d6d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dceb155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT-компании                       4388\n",
       "Разработка веб-сайтов *           2145\n",
       "Информационная безопасность *     2029\n",
       "Настройка Linux *                 1730\n",
       "Компьютерное железо               1545\n",
       "Гаджеты                           1371\n",
       "Социальные сети и сообщества      1177\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "df = pd.read_csv('puck.csv')\n",
    "df['tag'].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77aa7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df['brave_new_text'] != 'removed by selfcensorship ']\n",
    "# df = df[df['brave_new_text'] != 'сообщение удалено ']\n",
    "# df = df[df['brave_new_text'] != 'nan']\n",
    "# df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44028da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, y_train, y_test = train_test_split(df.brave_new_textpunc_stopwords, df.tag,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad900cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['brave_new_text'][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f64f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "# model = BertModel.from_pretrained('bert-base-multilingual-cased',\n",
    "#                                   output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "#                                   )\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "def embed(text):\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    \n",
    "    # Add the special tokens.\n",
    "    marked_text = \"[CLS] \" + str(text)  + \" [SEP]\"\n",
    "    # Split the sentence into tokens.\n",
    "#     tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    tokenized_text = tokenizer.encode(marked_text,padding=True, truncation=True,max_length=512)\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#     # Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "#     segments_ids = [1] * len(tokenized_text)\n",
    "#     # Convert inputs to PyTorch tensors\n",
    "#     tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#     segments_tensors = torch.tensor([segments_ids])\n",
    "    # Load pre-trained model (weights)\n",
    "    # # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "#         outputs = model(tokens_tensor, segments_tensors)\n",
    "        outputs=model(tokenized_text)\n",
    "        hidden_states = outputs[2]\n",
    "        # Each layer in the list is a torch tensor.\n",
    "    # Concatenate the tensors for all layers. We use `stack` here to\n",
    "    # create a new dimension in the tensor.\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # Remove dimension 1, the \"batches\".\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # Swap dimensions 0 and 1.\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "    # `token_vecs` is a tensor with shape [22 x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Stores the token vectors, with shape [22 x 768]\n",
    "#     token_vecs_sum = []\n",
    "#     # `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "#     # For each token in the sentence...\n",
    "#     for token in token_embeddings:\n",
    "#         # `token` is a [12 x 768] tensor\n",
    "#         # Sum the vectors from the last four layers.\n",
    "#         sum_vec = torch.sum(token[-4:], dim=0)\n",
    "#         # Use `sum_vec` to represent `token`.\n",
    "#         token_vecs_sum.append(sum_vec)\n",
    "# #     print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
    "# #     sentence_vector = np.mean(token_vecs_sum,axis=0)\n",
    "#     print(token_vecs.size())\n",
    "#     print(sentence_embedding.size())\n",
    "    return sentence_embedding\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6df164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.84503493e-02  2.48572677e-02  4.52326089e-02 -7.95979351e-02\n",
      " -1.53341526e-02  1.95008032e-02  6.84027299e-02 -2.58912588e-03\n",
      " -8.90688151e-02 -7.47816935e-02  5.33769056e-02  3.95353697e-02\n",
      "  2.15761587e-02  1.92577783e-02  8.65387917e-03  1.72012728e-02\n",
      "  5.63668720e-02  1.71543378e-02  2.38393713e-02  1.00879058e-01\n",
      " -3.04934941e-03 -1.82165168e-02  1.61965322e-02  1.45853609e-02\n",
      "  8.97004232e-02  2.11753529e-02 -1.13322595e-02  1.37898307e-02\n",
      " -1.44742606e-02  5.83038069e-02 -9.02313814e-02 -7.45823458e-02\n",
      "  1.09646402e-01  2.36695036e-02 -2.57688332e-02  8.15490484e-02\n",
      "  1.42576511e-03 -1.82034336e-02 -1.73426867e-01 -3.17930132e-02\n",
      " -7.68856108e-02  8.63788500e-02  1.19000718e-01  5.18518779e-03\n",
      " -2.13312916e-02  4.75826487e-02  3.57612483e-02 -1.77896973e-02\n",
      "  7.05813617e-02 -6.07864633e-02  2.48750150e-02 -5.70905991e-02\n",
      "  1.31985303e-02 -8.18155780e-02 -1.96225286e-01  8.16287473e-02\n",
      "  1.14382379e-01  3.10905278e-02 -5.76074310e-02  1.15664303e-02\n",
      "  5.26937805e-02  6.90283775e-02 -2.31972821e-02  4.41516601e-02\n",
      "  2.62674745e-02  1.14292957e-01 -6.32378459e-02  3.22253704e-02\n",
      " -8.95162672e-02 -6.74476335e-03  3.24033312e-02  3.35392505e-02\n",
      " -3.69854015e-03 -6.33003265e-02 -5.10212258e-02  5.02248853e-02\n",
      " -3.31195667e-02  1.75971799e-02 -1.73394866e-02  2.08459795e-02\n",
      "  7.70589977e-04  4.31425199e-02  3.22565297e-03  4.53968681e-02\n",
      " -6.94159884e-03  3.01019289e-02  4.04445594e-03  2.66365474e-03\n",
      "  3.16147283e-02  7.20458478e-03  8.43254849e-02  2.94355322e-02\n",
      "  1.61320984e-01 -5.84965292e-03 -6.10699644e-03 -8.94531957e-04\n",
      " -6.96614664e-03  6.59584999e-02 -3.77252884e-02  2.65637580e-02\n",
      " -1.58084661e-03 -1.39727183e-02 -2.91929711e-02 -6.42752126e-02\n",
      " -2.87014600e-02 -7.98033848e-02 -3.65799479e-02 -1.13343727e-02\n",
      " -7.80004170e-03 -4.65919748e-02 -3.83697450e-02  1.63824204e-03\n",
      "  1.53704822e-01  1.61749776e-02  1.30760362e-02  4.99596559e-02\n",
      " -5.02885245e-02  5.96324680e-03  3.69480029e-02 -2.79659927e-02\n",
      "  3.57423760e-02 -8.56462196e-02 -2.78317593e-02  5.85559458e-02\n",
      " -4.96230973e-03 -4.68837917e-02 -2.03249287e-02  2.88389642e-02\n",
      " -1.25456154e-02  2.34149909e-03 -7.97887146e-02  1.31512154e-03\n",
      " -4.61477973e-03  5.88650294e-02  8.49349424e-03  3.24132033e-02\n",
      " -8.58993176e-03  3.91250476e-02  4.54984829e-02 -8.77270922e-02\n",
      "  3.25833336e-02 -1.67344473e-02  6.22783601e-02  9.90380906e-03\n",
      "  6.61572348e-03 -4.43703867e-02  8.64840001e-02  3.12102977e-02\n",
      " -1.63765829e-02  3.24640274e-02  3.46214287e-02  2.14679409e-02\n",
      " -3.80119607e-02 -6.08616695e-02  9.38823372e-02 -1.37323979e-03\n",
      " -2.23685391e-02  3.07203853e-03 -3.06902193e-02  2.46562343e-02\n",
      "  3.73472273e-02  4.48510759e-02  2.62873471e-02  3.51065248e-02\n",
      "  2.48406231e-02 -4.12904397e-02 -1.98905310e-03  2.56045181e-02\n",
      " -3.75545956e-02 -1.49247071e-05  8.95954296e-02  1.02321833e-01\n",
      " -7.75177479e-02  5.15677929e-02  4.07728404e-02  5.56997508e-02\n",
      " -3.12727951e-02 -3.34161222e-02  1.80468969e-02  4.24158312e-02\n",
      "  7.18013570e-02 -3.27488407e-02  3.12250145e-02 -6.41529728e-03\n",
      "  8.63311440e-02  5.06936088e-02 -9.63729341e-03 -3.86482961e-02\n",
      "  3.45475948e-03  3.35999243e-02 -2.39134878e-02  1.36868749e-02\n",
      " -8.02229792e-02  4.10970487e-02  1.11314833e-01 -3.55650336e-01\n",
      "  3.94797139e-03  1.70912474e-01 -1.45032499e-02  8.81145429e-03\n",
      " -1.06110228e-02  1.01862205e-02  9.01324768e-03 -1.15732932e-02\n",
      "  1.46312714e-01 -6.37163222e-02  1.09314390e-01  1.14358002e-02\n",
      " -3.11159194e-02 -4.69978079e-02 -2.59855092e-02 -8.49663541e-02\n",
      " -4.38420624e-02  4.72506918e-02 -2.97210626e-02 -1.45604938e-01\n",
      "  5.92074133e-02 -5.23197018e-02 -2.25066952e-02 -2.97205802e-02\n",
      "  4.00350206e-02  4.96220253e-02 -3.47311981e-02  2.78546344e-02\n",
      "  4.49340343e-02 -1.02680720e-01 -3.69548164e-02  4.15183417e-02\n",
      " -9.66564715e-02  4.40157726e-02 -7.28418380e-02 -7.17696548e-02\n",
      " -8.07460696e-02 -1.86235411e-03  5.74870296e-02 -1.06296958e-02\n",
      " -3.34221497e-02 -5.00220396e-02 -1.40914517e-02 -1.98500063e-02\n",
      " -1.04174465e-02  2.58648004e-02 -9.68017895e-03 -8.64649415e-02\n",
      " -3.19865569e-02  1.87927000e-02  9.55741480e-03 -3.65840271e-02\n",
      " -2.37423778e-02  2.18216367e-02 -6.36352077e-02  1.54253235e-02\n",
      " -6.46219552e-02 -5.57894260e-02  2.83598565e-02 -5.67667447e-02\n",
      " -8.69948491e-02 -2.59791221e-02 -2.23351996e-02  3.23204547e-02\n",
      "  4.89245243e-02  5.32679446e-02  2.21973639e-02  1.11880759e-02\n",
      " -1.39666528e-01  1.75623242e-02 -8.69992934e-03 -1.25347702e-02\n",
      "  1.35753769e-02 -7.92743731e-03  4.93885837e-02  4.01302688e-02\n",
      " -6.34738579e-02  6.13217875e-02  3.79927568e-02  1.71142817e-02\n",
      "  3.35496292e-02 -3.18265222e-02  3.28289866e-02 -6.28356189e-02\n",
      "  6.32865056e-02  2.53105219e-02 -3.59348729e-02  8.13574418e-02\n",
      "  2.14969572e-02 -3.26125957e-02 -4.46960665e-02  3.44281644e-02\n",
      " -2.76517272e-02 -5.14193103e-02 -1.60971489e-02  3.16532031e-02\n",
      "  4.65123840e-02  1.02381473e-02 -5.23232259e-02  6.56787157e-02\n",
      " -7.38724396e-02 -8.46492592e-03 -2.52996404e-02 -1.81409717e-02\n",
      " -3.30633558e-02 -2.24119350e-02  2.94833574e-02  9.54996869e-02\n",
      " -1.13596562e-02 -2.32962240e-02 -5.22570200e-02 -4.04514447e-02\n",
      " -5.90167940e-02  1.08546793e-01  4.24223579e-02 -2.09014770e-02]\n"
     ]
    }
   ],
   "source": [
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, max_length=512,return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "        return embeddings[0].cpu().numpy()\n",
    "    \n",
    "print(embed_bert_cls('привет мир', model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab2d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg_test\n",
    "# msg_test = msg_test.tolist()\n",
    "# msg_test = [x for x in msg_test if str(x) != 'nan']\n",
    "# len(msg_test)\n",
    "# print(\"a\"+str(msg_test[18])+\"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0937d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 226/3597 [00:09<03:11, 17.62it/s]"
     ]
    }
   ],
   "source": [
    "# X_test=[]\n",
    "# for text in tqdm.tqdm(msg_test):\n",
    "#     X_test.append(embed(text))\n",
    "# X_test_new=[]\n",
    "# for x in X_test:\n",
    "#     np_tensor = x.numpy()\n",
    "#     tf_tensor = tf.convert_to_tensor(np_tensor)\n",
    "#     X_test_new.append(tf_tensor)\n",
    "\n",
    "X_test=[]\n",
    "for text in tqdm.tqdm(msg_test):\n",
    "    X_test.append(embed_bert_cls(str(text), model, tokenizer))\n",
    "# X_test_new=[]\n",
    "# for x in X_test:\n",
    "# #     np_tensor = x.numpy()\n",
    "#     tf_tensor = tf.convert_to_tensor(x)\n",
    "#     X_test_new.append(tf_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new=[]\n",
    "for x in X_test:\n",
    "#     np_tensor = x.numpy()\n",
    "    tf_tensor = tf.convert_to_tensor(x)\n",
    "    X_test_new.append(tf_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09497b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg_train\n",
    "# msg_train = msg_train.tolist()\n",
    "# msg_train = [x for x in msg_train if str(x) != 'nan']\n",
    "# len(msg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=[]\n",
    "# for text in tqdm.tqdm(msg_train):\n",
    "#     X_train.append(embed(text))\n",
    "# X_train_new=[]\n",
    "# for x in X_train:\n",
    "#     np_tensor = x.numpy()\n",
    "#     tf_tensor = tf.convert_to_tensor(np_tensor)\n",
    "#     X_train_new.append(tf_tensor)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train=[]\n",
    "for text in tqdm.tqdm(msg_train):\n",
    "    X_train.append(embed_bert_cls(str(text), model, tokenizer))\n",
    "X_train_new=[]\n",
    "for x in X_train:\n",
    "#     np_tensor = x.numpy()\n",
    "    tf_tensor = tf.convert_to_tensor(x)\n",
    "    X_train_new.append(tf_tensor)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4876031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551404ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = embed(msg_test)\n",
    "# X_test.shape\n",
    "# # we don't have enough memory to apply embeddings in one shot,\n",
    "# # so we have to split the data into batches and concatenate them later\n",
    "# splits = np.array_split(msg_train, 5)\n",
    "# l = list()\n",
    "# for split in splits:\n",
    "#     l.append(embed(split))\n",
    "# X_train = tf.concat(l, axis=0)\n",
    "# del l\n",
    "# X_train.shape\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced', classes=['IT-компании  ',\n",
    "                                      'Разработка веб-сайтов * ',\n",
    "                                      'Компьютерное железо  ',\n",
    "                                      'Настройка Linux * ',\n",
    "                                      'Гаджеты  ',\n",
    "                                      'Информационная безопасность * ',\n",
    "                                      'Социальные сети и сообщества  '], y=y_train) #y_train)\n",
    "class_weight\n",
    "# initialize the model and assign weights to each class\n",
    "clf = SVC(class_weight={\"IT-компании  \":class_weight[0], \"Разработка веб-сайтов * \":class_weight[1],\n",
    "                        \"Компьютерное железо  \":class_weight[2], \"Настройка Linux * \":class_weight[3],\n",
    "                        \"Гаджеты  \":class_weight[4],  \"Информационная безопасность * \":class_weight[5],\n",
    "                        \"Социальные сети и сообщества  \":class_weight[6]})\n",
    "# train the model\n",
    "clf.fit(X_train_new, y_train) #X_train,y_train\n",
    "# use the model to predict the testing instances\n",
    "y_pred = clf.predict(np.array(X_test_new))\n",
    "# generate the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(class_weight={\"Evidence\":class_weight[0], \"Concluding Statement\":class_weight[1],\n",
    "#                         \"Position\":class_weight[2], \"Claim\":class_weight[3],\n",
    "#                         \"Counterclaim\":class_weight[4],  \"Lead\":class_weight[5],\n",
    "#                         \"Rebuttal\":class_weight[6]})\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(np.array(X_test))\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.utils import class_weight\n",
    "# from sklearn.preprocessing import *\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# sample = class_weight.compute_sample_weight('balanced', y_train)\n",
    "\n",
    "# #Classifier Naive Bayes\n",
    "# naive = MultinomialNB()\n",
    "# naive.fit(X_train,y_train, sample_weight=sample)\n",
    "# predictions_NB = naive.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# num_words = 10000\n",
    "# max_news_len = 30\n",
    "# nb_classes=7\n",
    "# # model_lstm = Sequential()\n",
    "# # model_lstm.add(Embedding(num_words,32,input_length=max_news_len))\n",
    "# # model_lstm.add(LSTM(16))\n",
    "# # model_lstm.add(Dense(7,activation='softmax'))\n",
    "# # model_lstm.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# # model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9504fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(class_weight={\"IT-компании  \":class_weight[0], \"Разработка веб-сайтов * \":class_weight[1],\n",
    "                        \"Компьютерное железо  \":class_weight[2], \"Настройка Linux * \":class_weight[3],\n",
    "                        \"Гаджеты  \":class_weight[4],  \"Информационная безопасность * \":class_weight[5],\n",
    "                        \"Социальные сети и сообщества  \":class_weight[6]})\n",
    "clf.fit(X_train_new, y_train)\n",
    "y_pred = clf.predict(np.array(X_test_new))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369038cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_new)\n",
    "X_test = scaler.transform(X_test_new)\n",
    "\n",
    "sample = class_weight.compute_sample_weight('balanced', y_train)\n",
    "\n",
    "#Classifier Naive Bayes\n",
    "naive = MultinomialNB()\n",
    "naive.fit(X_train,y_train, sample_weight=sample)\n",
    "predictions_NB = naive.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32b7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"qwe.csv\")\n",
    "df = df.applymap(str)\n",
    "df['tag'] = df['tag'].map({\"IT-компании  \": 0, \"Разработка веб-сайтов * \": 1,\n",
    "                        \"Компьютерное железо  \": 2, \"Настройка Linux * \": 3,\n",
    "                        \"Гаджеты  \": 4,  \"Информационная безопасность * \": 5,\n",
    "                        \"Социальные сети и сообщества  \": 6})\n",
    "df.head(4)\n",
    "num_classes=7\n",
    "y = tf.keras.utils.to_categorical(df[\"tag\"].values, num_classes=7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.brave_new_textpunc_stopwords, y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136e35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35985af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e8a768",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2292/2366345422.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dropout\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pooled_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# preprocessor = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "# encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1\")\n",
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "x = preprocessor(i)\n",
    "x = encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
    "model = tf.keras.Model(i, x)\n",
    "n_epochs = 1\n",
    "METRICS = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")]\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
    "                                                      patience = 3,\n",
    "                                                      restore_best_weights = True)\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)\n",
    "\n",
    "\n",
    "PATH_TO_MODELS = 'C:\\Program Files\\modelb\\model_0epochs.pb'\n",
    "for epoch in range(n_epochs):\n",
    "    model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs = 1,\n",
    "                      validation_data = (X_test, y_test),\n",
    "                      callbacks = [earlystop_callback])\n",
    "    save_name = 'model_%sepochs.pb' % str(epoch)\n",
    "    model.save(os.path.join(PATH_TO_MODELS, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fa61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('qwe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331/331 [==============================] - 1879s 6s/step - loss: 0.6194 - accuracy: 0.7826 - val_loss: 0.6653 - val_accuracy: 0.7622\n",
      "331/331 [==============================] - 1670s 5s/step - loss: 0.6126 - accuracy: 0.7839 - val_loss: 0.6693 - val_accuracy: 0.7608\n",
      "331/331 [==============================] - 1635s 5s/step - loss: 0.6180 - accuracy: 0.7837 - val_loss: 0.6646 - val_accuracy: 0.7602\n",
      " 71/331 [=====>........................] - ETA: 15:55 - loss: 0.6111 - accuracy: 0.7830"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "n_epochs = 20\n",
    "\n",
    "model = tf.keras.models.load_model('C:\\Program Files\\modelb\\model_17epochs.h5',custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "for epoch in range(n_epochs):\n",
    "    model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs = 1,\n",
    "                      validation_data = (X_test, y_test)\n",
    "                      )\n",
    "    save_name = 'model_%sepochs.h5' % str(epoch)\n",
    "    model.save(os.path.join('C:\\Program Files\\modelb', save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc118172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679912ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f04d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1672c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeding(sentences):\n",
    "    preprocessed_text = bert_preprocess(sentences)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar  = embed_bert_cls('привет мир', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.convert_to_tensor(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = embed_bert_cls(str(text_input), model, tokenizer)\n",
    "tensor = tf.convert_to_tensor(outputs)\n",
    "# tf.reshape(tensor,[1,312])\n",
    "tensor =  tf.expand_dims(tensor, axis=0)\n",
    "tensor =  tf.expand_dims(tensor, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
